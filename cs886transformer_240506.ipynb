{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53cc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.func as func\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import torchtext.data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import spacy\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "771daacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cpu\"\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "256dc245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16, 20])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return torch.sum(x)\n",
    "\n",
    "a=torch.tensor([[[1,2],[3,4]],\n",
    "                [[5,6],[7,8]]])\n",
    "b=func.vmap(f,2)(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c65bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout):\n",
    "        super(Attention,self).__init__()\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//nhead\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_o = nn.Linear(self.d_model, self.d_model)\n",
    "        self.elu = nn.ELU()\n",
    "        \n",
    "    def forward(self, Q, K, V, pad_mask=None, attn_mask=None):\n",
    "        Q=self.W_q(Q)\n",
    "        K=self.W_k(K)\n",
    "        V=self.W_v(V)\n",
    "        \n",
    "        Q = Q.reshape(Q.shape[0], Q.shape[1], self.nhead, self.d_k).transpose(0, 2)\n",
    "        K = K.reshape(K.shape[0], K.shape[1], self.nhead, self.d_k).transpose(0, 2)\n",
    "        V = V.reshape(V.shape[0], V.shape[1], self.nhead, self.d_k).transpose(0, 2)\n",
    "        #shape=(heads, batches, seqlen, dims)\n",
    "        \n",
    "        '''\n",
    "        X = torch.empty(Q.shape).to(DEVICE)\n",
    "        \n",
    "        def phi(x):\n",
    "            return self.elu(x)+1\n",
    "        \n",
    "        \n",
    "        for head in range(Q.shape[0]):\n",
    "            for batch in range(Q.shape[1]):\n",
    "                for i in range(Q.shape[2]):\n",
    "                    numer=0\n",
    "                    denom=torch.zeros(V.shape[3]).to(DEVICE)\n",
    "                    for j in range(V.shape[2]):\n",
    "                        if not(\n",
    "                            ((pad_mask is not None) and (pad_mask[batch,j])) \n",
    "                            or ((attn_mask is not None) and (attn_mask[i,j]))\n",
    "                        ):\n",
    "                            numer+=torch.dot(phi(K[head,batch,j]), V[head,batch,j])\n",
    "                            denom+=V[head,batch,j]\n",
    "                    phiQ=phi(Q[head,batch,i])\n",
    "                    X[head,batch,i]=phiQ*numer/torch.dot(phiQ,denom)\n",
    "                            \n",
    "        '''\n",
    "        X = torch.matmul(Q, K.transpose(2,3)) / math.sqrt(self.d_k)\n",
    "        if (pad_mask is not None):\n",
    "            #shape=(batches, seqlen)\n",
    "            X=X.masked_fill(pad_mask.reshape(1,pad_mask.shape[0],1,pad_mask.shape[1]), -1.0e10)\n",
    "        \n",
    "        if (attn_mask is not None):\n",
    "            #shape=(seqlen, seqlen)\n",
    "            X=X.masked_fill(attn_mask.reshape(1,1,attn_mask.shape[0],attn_mask.shape[1]), -1.0e10)\n",
    "        \n",
    "        X = torch.softmax(X,dim=3)\n",
    "        X = torch.matmul(X, V)\n",
    "        #'''\n",
    "        X = X.transpose(0,2)\n",
    "        X = X.reshape(X.shape[0],X.shape[1],X.shape[2]*X.shape[3])\n",
    "        X = self.W_o(X)\n",
    "        return X\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mha = Attention(d_model, nhead, dropout)\n",
    "        #self.mha = nn.MultiheadAttention(d_model,nhead)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1=nn.LayerNorm(d_model)\n",
    "        self.norm2=nn.LayerNorm(d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X, src_pad_mask=None):\n",
    "        sub_X = self.mha(X,X,X,src_pad_mask)\n",
    "        X=self.norm1(X+self.dropout(sub_X))\n",
    "        sub_X=self.feed_forward(X)\n",
    "        X=self.norm2(X+self.dropout(sub_X))\n",
    "        return X\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.mha = Attention(d_model, nhead, dropout)\n",
    "        self.masked_mha = Attention(d_model, nhead, dropout)\n",
    "        #self.mha = nn.MultiheadAttention(d_model,nhead)\n",
    "        #self.masked_mha = nn.MultiheadAttention(d_model,nhead)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff,d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, X, memory, src_pad_mask=None, trg_pad_mask=None):\n",
    "        sub_X=self.masked_mha(X,X,X, trg_pad_mask, \n",
    "                              attn_mask=(1-torch.tril(torch.ones(X.shape[0],X.shape[0]))).bool().to(DEVICE))\n",
    "        X=self.norm1(X+self.dropout(sub_X))\n",
    "        sub_X = self.mha(X, memory, memory, src_pad_mask)\n",
    "        X=self.norm2(X+self.dropout(sub_X))\n",
    "        sub_X = self.feed_forward(X)\n",
    "        X=self.norm3(X+self.dropout(sub_X))\n",
    "        return X\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model,max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        PE=torch.empty((max_len,d_model))\n",
    "        pos=torch.arange(max_len).reshape((max_len,1))\n",
    "        wave_len=10000**(torch.arange(0,d_model,step=2)/d_model)\n",
    "        PE[:,0::2]=torch.sin(pos*wave_len)\n",
    "        PE[:,1::2]=torch.cos(pos*wave_len)\n",
    "        self.register_buffer('PE', PE) #to run on gpu\n",
    "    def forward(self, X):\n",
    "        return X+self.PE[0:len(X),].reshape((len(X),1,X.shape[2]))\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_vocab_size,trg_vocab_size,\n",
    "                 n_encoders=6,n_decoders=6,d_model=512,nhead=8,d_ff=2048,dropout= 0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.trg_tok_emb = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.encoder = nn.ModuleList([deepcopy(Encoder(\n",
    "            d_model,nhead,d_ff,dropout\n",
    "        )) for i in range(n_encoders)])\n",
    "        self.decoder = nn.ModuleList([deepcopy(Decoder(\n",
    "            d_model,nhead,d_ff,dropout\n",
    "        )) for i in range(n_decoders)])\n",
    "        self.generator = nn.Linear(d_model, trg_vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "    def forward(self,src,trg):\n",
    "        memory = self.src_tok_emb(src)\n",
    "        memory=self.positional_encoding(memory)\n",
    "        output = self.trg_tok_emb(trg)\n",
    "        output=self.positional_encoding(output)\n",
    "        src_pad_mask=(src == PAD_IDX).transpose(0,1)\n",
    "        trg_pad_mask=(trg == PAD_IDX).transpose(0,1)\n",
    "        \n",
    "        \n",
    "        for encoder in self.encoder:\n",
    "            memory=encoder(memory,src_pad_mask)\n",
    "        for decoder in self.decoder:\n",
    "            output = decoder(output,\n",
    "                             memory,\n",
    "                             src_pad_mask,trg_pad_mask)\n",
    "\n",
    "        return self.generator(output)\n",
    "    \n",
    "    def encode(self,src):\n",
    "        memory = self.src_tok_emb(src)\n",
    "        memory=self.positional_encoding(memory)\n",
    "        for encoder in self.encoder:\n",
    "            memory=encoder(memory)\n",
    "        return memory\n",
    "\n",
    "    def decode(self,trg,memory):\n",
    "        output = self.trg_tok_emb(trg)\n",
    "        output=self.positional_encoding(output)\n",
    "        for decoder in self.decoder:\n",
    "            output = decoder(output,\n",
    "                             memory)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af82fdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b3a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constants\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3 #special token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb742d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "#downloaded from https://www.kaggle.com/datasets/devicharith/language-translation-englishfrench\n",
    "dat=pd.read_csv(\"eng_-french.csv\") \n",
    "dat_en=dat[\"English words/sentences\"].to_numpy()[0:1000]\n",
    "dat_fr=dat[\"French words/sentences\"].to_numpy()[0:1000]\n",
    "print(len(dat_en))\n",
    "\n",
    "#get tokenizers\n",
    "spacy_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "spacy_fr = get_tokenizer('spacy', language='fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40de757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31d5d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vocabularies and text tokenizers\n",
    "def get_vocab_text(dat,tokenizer):\n",
    "    counter = Counter()\n",
    "    for s in dat:\n",
    "        counter.update(list(tokenizer(s)))\n",
    "    vocab = build_vocab_from_iterator([counter], specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    def text_tok(text):\n",
    "        str_tokens=tokenizer(text)\n",
    "        int_tokens=vocab(str_tokens)\n",
    "        return torch.cat((torch.tensor([BOS_IDX]),torch.tensor(int_tokens),torch.tensor([EOS_IDX])))\n",
    "    return vocab,text_tok\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf77354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  2, 353,   0,   6,   3])\n"
     ]
    }
   ],
   "source": [
    "vocab_en,text_en=get_vocab_text(dat_en,spacy_en)\n",
    "print(text_en(\"hello world!\"))\n",
    "vocab_fr,text_fr=get_vocab_text(dat_fr,spacy_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e277f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d41a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, src, trg):\n",
    "        super().__init__()\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.trg[idx]\n",
    "\n",
    "def get_dataloader(dat_src,dat_trg,text_tok_src,text_tok_trg,train_split=0.8):\n",
    "    dat_st=CustomDataset(dat_src,dat_trg)\n",
    "    train_data, val_data = random_split(dat_st, [train_split, 1-train_split])\n",
    "    def collate_fn(batch):\n",
    "        src_batch, trg_batch = [], []\n",
    "        for src,trg in batch:\n",
    "            src_batch.append(text_tok_src(src))\n",
    "            trg_batch.append(text_tok_trg(trg))\n",
    "        return pad_sequence(src_batch, padding_value=PAD_IDX), pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=16, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=16, collate_fn=collate_fn)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d727fef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloader(dat_en,dat_fr,text_en,text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "711b5f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(transformer, train_dataloader, val_dataloader,  criterion, optimizer, plt_title, epochs=5):\n",
    "    start=timer()\n",
    "    train_loss=np.zeros(epochs)\n",
    "    val_loss=np.zeros(epochs)\n",
    "    for epoch in range(epochs):\n",
    "        #print(f\"epoch {epoch}\")\n",
    "        transformer.train()\n",
    "\n",
    "        for src, trg in train_dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            out = transformer(src, trg[:-1, :])\n",
    "            out = out.reshape(out.shape[0]*out.shape[1],out.shape[2])\n",
    "            loss = criterion(out, trg[1:, :].flatten())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            train_loss[epoch] += loss.item()\n",
    "            optimizer.step()\n",
    "        #print(f\"training loss: {train_loss/len(list(train_dataloader))}\")\n",
    "        #print(f\"time: {timer()-start}\")\n",
    "\n",
    "        transformer.eval()\n",
    "\n",
    "        for src, trg in val_dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "            out = transformer(src, trg[:-1, :])\n",
    "            out = out.reshape(out.shape[0]*out.shape[1],out.shape[2])\n",
    "            loss = criterion(out, trg[1:, :].flatten())\n",
    "            val_loss[epoch] += loss.item()\n",
    "\n",
    "        #print(f\"validation loss: {val_loss/len(list(val_dataloader))}\")\n",
    "        print(f\"time: {timer()-start}\")\n",
    "    \n",
    "    train_loss/=len(list(train_dataloader))\n",
    "    val_loss/=len(list(val_dataloader))\n",
    "    print(train_loss)\n",
    "    print(val_loss)\n",
    "    #'''\n",
    "    plt.plot(range(epochs),train_loss,label=\"training\")\n",
    "    plt.plot(range(epochs),val_loss,label=\"validation\")\n",
    "    plt.title(plt_title)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylim([0,0.1+np.max([train_loss,val_loss])])\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    #'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75bc6296",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mPAD_IDX)\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(transformer\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m----> 4\u001b[0m train(transformer, train_dataloader,val_dataloader,\n\u001b[0;32m      5\u001b[0m       criterion, optimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(transformer, train_dataloader, val_dataloader, criterion, optimizer, plt_title, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, trg[\u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m train_loss[epoch] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, train_dataloader,val_dataloader,\n",
    "      criterion, optimizer, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff036478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src):\n",
    "    src=text_en(src)\n",
    "    src=src.reshape(len(src),1).to(DEVICE)\n",
    "    trg=[BOS_IDX]\n",
    "    memory = model.encode(src)\n",
    "    for i in range(len(src)+10):\n",
    "        out=model.decode(torch.tensor(trg).reshape(len(trg),1).to(DEVICE), memory)\n",
    "        prob=model.generator(out[i])\n",
    "        next_word=torch.argmax(prob)\n",
    "        trg+=[next_word]\n",
    "        if next_word==EOS_IDX:\n",
    "            break\n",
    "    trg=vocab_fr.lookup_tokens(trg)\n",
    "    return trg[1:(len(trg)-1)]\n",
    "\n",
    "def evaluate(model, test_src, test_trg, samp_size, max_n, verbose=False):\n",
    "    samp_size = np.min((samp_size,len(test_src)))\n",
    "    score=np.empty(samp_size)\n",
    "    samp=np.random.choice(len(test_src), samp_size, replace=False)\n",
    "    for i in range(samp_size):\n",
    "        idx=samp[i]\n",
    "        pred=translate(model,test_src[idx])\n",
    "        if (verbose):\n",
    "            print(test_trg[idx])\n",
    "            print(\" \".join(pred))\n",
    "        score[i]=bleu_score([pred], [[spacy_fr(test_trg[idx])]],max_n=max_n,weights=np.full(max_n,1/max_n))\n",
    "    return np.mean(score),np.std(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2202d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloaded from https://huggingface.co/datasets/Nicolas-BZRD/Parallel_Global_Voices_English_French\n",
    "dat1=pd.read_parquet(\"eng_-french2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e216cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1_en=dat1[\"en\"].to_numpy()\n",
    "dat1_fr=dat1[\"fr\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cf5736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_layers(n_encoders,n_decoders):\n",
    "    transformer = Transformer(len(vocab_en), len(vocab_fr),n_encoders,n_decoders, 512, 8,  2048, 0.1).to(DEVICE)\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    optimizer = torch.optim.Adam(transformer.parameters())\n",
    "    train(transformer, train_dataloader,val_dataloader,\n",
    "          criterion, optimizer, \"{} encoders, {} decoders\".format(n_encoders,n_decoders))\n",
    "    print(evaluate(transformer, dat_en, dat_fr, 1000, 4))\n",
    "    print(evaluate(transformer, dat1_en, dat1_fr, 1000, 4))\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3a0d937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 221.58196280000266\n",
      "time: 419.205575400003\n",
      "time: 621.7796546000027\n",
      "time: 825.2778282000072\n",
      "time: 1034.022650400002\n",
      "[2.95854954 2.09369774 1.76436064 1.54349631 1.39737407]\n",
      "[2.37184895 2.06233671 1.91164701 1.85753546 1.79414701]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.22902752559405745, 0.33692915512524246)\n",
      "(0.011329959106784734, 0.06807507226134787)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer=tune_layers(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77536a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m translate(transformer,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformer' is not defined"
     ]
    }
   ],
   "source": [
    "translate(transformer,\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdcb905",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_layers(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d85169",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_layers(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d84121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_src, test_trg, samp_size, max_n, verbose=False):\n",
    "    samp_size = np.min((samp_size,len(test_src)))\n",
    "    score=np.empty(samp_size)\n",
    "    samp=np.random.choice(len(test_src), samp_size, replace=False)\n",
    "    for i in range(samp_size):\n",
    "        idx=samp[i]\n",
    "        pred=translate(model,test_src[idx])\n",
    "        if (verbose):\n",
    "            print(test_trg[idx])\n",
    "            print(\" \".join(pred))\n",
    "        score[i]=bleu_score([pred], [spacy_fr(test_trg[idx])],max_n=max_n,weights=np.full(max_n,1/max_n))\n",
    "    return np.mean(score),np.std(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ecf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(pred, test_trg, max_n, verbose=False):\n",
    "    samp_size = len(pred)\n",
    "    for i in range(samp_size):\n",
    "        score[i]=bleu_score([pred[i]], [spacy_fr(test_trg[i])],max_n=max_n,weights=np.full(max_n,1/max_n))\n",
    "    return np.mean(score),np.std(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73319077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda\"\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"1 encoders, 1 decoders\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf0=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1, void_mha=True).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"No multi-head attention\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf1=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1, void_mask=True).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"No masks\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf2=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1, void_pe=True).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"No positional encoding\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf3=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ebeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1, void_ff=True).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"No feed-forward layers\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf4=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1, void_ann=True).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "train(transformer, criterion, optimizer, \"No residual connection+layer normalization\")\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat_en, dat_fr, 1000, 2))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 1))\n",
    "print(evaluate(transformer, dat1_en, dat1_fr, 1000, 2))\n",
    "tf5=transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a2b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat3_en=[\"A transformer is a deep learning architecture developed by Google and based on the multi-head attention mechanism, proposed in a 2017 paper \\\"Attention Is All You Need\\\".\",\n",
    "\n",
    "\"Text is converted to numerical representations called tokens, and each token is converted into a vector via looking up from a word embedding table.\",\n",
    "\n",
    "\"At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism allowing the signal for key tokens to be amplified and less important tokens to be diminished.\",\n",
    "\n",
    "\"The transformer paper, published in 2017, is based on the softmax-based attention mechanism proposed by Bahdanau et. al. in 2014 for machine translation, and the Fast Weight Controller, similar to a transformer, proposed in 1992.\",\n",
    "\n",
    "\"Transformers have the advantage of having no recurrent units, and thus requires less training time than previous recurrent neural architectures, such as long short-term memory (LSTM), and its later variation has been prevalently adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.\",\n",
    "\n",
    "\"This architecture is now used not only in natural language processing and computer vision, but also in audio and multi-modal processing.\",\n",
    "\n",
    "\"It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).\"]\n",
    "\n",
    "dat3_fr=[\"Un transformateur est une architecture d’apprentissage profond développée par Google et basée sur le mécanisme d’attention multi-têtes, proposé dans un article de 2017 \\\"Attention est tout ce dont vous avez besoin\\\".\",\n",
    "\n",
    "\"Le texte est converti en représentations numériques appelées jetons, et chaque jeton est converti en vecteur en remontant vers le haut à partir d’une table d’intégration de mots.\",\n",
    "\n",
    "\"À chaque couche, chaque jeton est ensuite contextualisé dans le cadre de la fenêtre contextuelle avec d’autres jetons (non masqués) via un mécanisme d’attention multi-têtes parallèle permettant d’amplifier le signal pour les jetons clés et de diminuer les jetons moins importants.\",\n",
    "\n",
    "\"L’article sur le transformateur, publié en 2017, est basé sur le mécanisme d’attention basé sur softmax proposé par Bahdanau et al. en 2014 pour la traduction automatique, et le contrôleur de poids rapide, similaire à un transformateur, proposé en 1992.\",\n",
    "\n",
    "\"Les transformateurs ont l’avantage de ne pas avoir d’unités récurrentes, et nécessitent donc moins de temps d’entraînement que les architectures neuronales récurrentes précédentes, telles que la mémoire à long terme et à court terme (LSTM), et sa variante ultérieure a été largement adoptée pour l’entraînement de grands modèles de langage (LLM) sur de grands ensembles de données (de langage), tels que le corpus Wikipédia et le Common Crawl.\",\n",
    "\n",
    "\"Cette architecture est aujourd’hui utilisée non seulement dans le traitement du langage naturel et la vision par ordinateur, mais aussi dans l’audio et le traitement multimodal.\",\n",
    "\n",
    "\"Cela a également conduit au développement de systèmes pré-entraînés, tels que les transformateurs génératifs pré-entraînés (GPT) et BERT (Représentations bidirectionnelles d'encodeurs à partir de transformateurs).\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer in [tf0,tf1,tf2,tf3,tf4,tf5]:\n",
    "    print(\"*\")\n",
    "    print(evaluate(transformer, dat_en, dat_fr, 1000, 3))\n",
    "    print(evaluate(transformer, dat_en, dat_fr, 1000, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer in [tf0,tf1,tf2,tf3,tf4,tf5]:\n",
    "    print(\"*\")\n",
    "    #print(evaluate(transformer, dat3_en, dat3_fr, 1000, 1))\n",
    "    #print(evaluate(transformer, dat3_en, dat3_fr, 1000, 2))\n",
    "    print(\" \".join(translate(transformer, \"Meet me at the hotel bar.\")))\n",
    "    print(\" \".join(translate(transformer, \"Money won't be a problem.\")))\n",
    "    print(\" \".join(translate(transformer, \"We met in a coffee shop near the campus.\")))\n",
    "    print(\" \".join(translate(transformer, \"Hey, what do you think you're doing?\")))\n",
    "    print(\" \".join(translate(transformer, \"They offered no explanation.\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2aa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2_en=[\"I am.\",\"You are.\",\"He is.\",\"She is.\",\"We are.\",\"They are.\"]\n",
    "dat2_fr=[\"Je suis.\",\"Tu est.\",\"Il est.\",\"Elle est.\",\"Nous sommes.\",\"Ils sont.\"]\n",
    "for transformer in [tf0,tf1,tf2,tf3,tf4,tf5]:\n",
    "    print(\"*\")\n",
    "    print(evaluate(transformer, dat2_en, dat2_fr, 1000, 1, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec24617",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "transformer32 = Transformer(len(vocab_en), len(vocab_fr),3,2, 512, 8,  2048, 0.1).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer32.parameters())\n",
    "train(transformer32, criterion, optimizer, \"3 encoders, 2 decoders\")\n",
    "\n",
    "transformer11 = Transformer(len(vocab_en), len(vocab_fr),1,1, 512, 8,  2048, 0.1).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer11.parameters())\n",
    "train(transformer11, criterion, optimizer, \"1 encoders, 1 decoders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp=np.random.choice(len(dat_en), 5)\n",
    "samp1=np.random.choice(len(dat1_en), 5)\n",
    "dat2_en=[\"I am.\",\"You are.\",\"He is.\",\"She is.\",\"We are.\",\"They are.\"]\n",
    "dat2_fr=[\"Je suis.\",\"Tu est.\",\"Il est.\",\"Elle est.\",\"Nous sommes.\",\"Ils sont.\"]\n",
    "for i in samp:\n",
    "    print(dat_en[i])\n",
    "for i in samp1:\n",
    "    print(dat1_en[i])\n",
    "for i in dat2_en:\n",
    "    print(i)\n",
    "for i in samp:\n",
    "    print(dat_fr[i])\n",
    "for i in samp1:\n",
    "    print(dat1_fr[i])\n",
    "for i in dat2_fr:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2_en=[\"I am.\",\"You are.\",\"He is.\",\"She is.\",\"We are.\",\"They are.\"]\n",
    "dat2_fr=[\"Je suis.\",\"Tu est.\",\"Il est.\",\"Elle est.\",\"Nous sommes.\",\"Ils sont.\"]\n",
    "for i in samp:\n",
    "    print(dat_en[i])\n",
    "for i in samp1:\n",
    "    print(dat1_en[i])\n",
    "for i in dat2_en:\n",
    "    print(i)\n",
    "for i in samp:\n",
    "    print(dat_fr[i])\n",
    "for i in samp1:\n",
    "    print(dat1_fr[i])\n",
    "for i in dat2_fr:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da2e842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in samp:\n",
    "    print(\" \".join(translate(transformer32,dat_en[i])))\n",
    "for i in samp1:\n",
    "    print(\" \".join(translate(transformer32,dat1_en[i])))\n",
    "for i in range(6):\n",
    "    print(\" \".join(translate(transformer32,dat2_en[i])))\n",
    "# According to Google Translate, the output translates to:\n",
    "'''\n",
    "I am really happy .\n",
    "I am really happy .\n",
    "It's a good story .\n",
    "I am really happy .\n",
    "It was a little hot.\n",
    "It was a little hot.\n",
    "I am really happy .\n",
    "I am really happy .\n",
    "It was a little hot.\n",
    "I am really happy .\n",
    "It's a good story .\n",
    "It's a good .\n",
    "It's a good .\n",
    "It's a good .\n",
    "It's a good .\n",
    "It's a good .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da93a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=0\n",
    "for i in samp:\n",
    "    s=(bleu_score([translate(transformer32,dat_en[i])], [spacy_fr(dat_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "\n",
    "for i in samp1:\n",
    "    s=(bleu_score([translate(transformer32,dat1_en[i])], [spacy_fr(dat1_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "\n",
    "for i in range(6):\n",
    "    s=(bleu_score([translate(transformer32,dat2_en[i])], [spacy_fr(dat2_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "print(score/16)\n",
    "\n",
    "score=0\n",
    "for i in samp:\n",
    "    s=(bleu_score([translate(transformer32,dat_en[i])], [spacy_fr(dat_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "\n",
    "for i in samp1:\n",
    "    s=(bleu_score([translate(transformer32,dat1_en[i])], [spacy_fr(dat1_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "\n",
    "for i in range(6):\n",
    "    s=(bleu_score([translate(transformer32,dat2_en[i])], [spacy_fr(dat2_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "print(score/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee5c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in samp:\n",
    "    print(\" \".join(translate(transformer11,dat_en[i])))\n",
    "for i in samp1:\n",
    "    print(\" \".join(translate(transformer11,dat1_en[i])))\n",
    "for i in range(6):\n",
    "    print(\" \".join(translate(transformer11,dat2_en[i])))\n",
    "# According to Google Translate, the output translates to:\n",
    "'''\n",
    "I can't force you to do this.\n",
    "You think you're old, don't you?\n",
    "How would you say such a thing?\n",
    "I understand it, but I don't agree with it.\n",
    "He's a good singer for a win.\n",
    "The general calmed down because of my teeth.\n",
    "He has news for the Tom is Crazy doc.\n",
    "Without water, girls have girls at four o'clock.\n",
    "Even the adults and let's see the French, it was last.\n",
    "But later, Tom, the hours, the exams, I'm done.\n",
    "I'm fussy.\n",
    "You are devoid of that.\n",
    "He's addicted.\n",
    "She is caring.\n",
    "We are drunk.\n",
    "They are expensive.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=0\n",
    "for i in samp:\n",
    "    s=(bleu_score([translate(transformer11,dat_en[i])], [spacy_fr(dat_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "\n",
    "for i in samp1:\n",
    "    s=(bleu_score([translate(transformer11,dat1_en[i])], [spacy_fr(dat1_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "\n",
    "for i in range(6):\n",
    "    s=(bleu_score([translate(transformer11,dat2_en[i])], [spacy_fr(dat2_fr[i])],max_n=1,weights=np.full(1,1)))\n",
    "    print(s)\n",
    "    score+=s\n",
    "print(score/16)\n",
    "\n",
    "score=0\n",
    "for i in samp:\n",
    "    s=(bleu_score([translate(transformer11,dat_en[i])], [spacy_fr(dat_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "\n",
    "for i in samp1:\n",
    "    s=(bleu_score([translate(transformer11,dat1_en[i])], [spacy_fr(dat1_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "\n",
    "for i in range(6):\n",
    "    s=(bleu_score([translate(transformer11,dat2_en[i])], [spacy_fr(dat2_fr[i])],max_n=2,weights=np.full(2,1/2)))\n",
    "    score+=s\n",
    "print(score/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_en=[\"Je ne peux pas vous permettre de faire cela.\",\n",
    "\"Tu me trouves moche, n'est-ce pas?\",\n",
    "\"Comment oses-tu dire une chose pareille?\",\n",
    "\"Je le comprends mais je ne suis tout de même pas d'accord.\",\n",
    "\"C'est une excellente journée pour une randonnée.\",\n",
    "\"Le modérateur était David Ignatius du Washington Post.\",\n",
    "\"Il appartient à la tribu Mehsud, dans la région politiquement instable du Sud-Waziristan et dirige dans les 20 000 miliciens pro-talibans.\",\n",
    "\"A cause du manque d'eau, les filles doivent marcher longtemps tout en transportant de lourds bidons d'eau.\",\n",
    "\"Même le Ghana et le Laos nous ont surpassés l'an dernier.\",\n",
    "\"Mais à partir du mardi 14 avril 2009, Neptune va progressivement se retirer de Lakhana Duang Muang pour se rapprocher de la droite du soleil.  Cela signifie que le mauvais présage ou les difficultés concernant Bangkok vont s'estomper.\",\n",
    "\"Je suis.\",\n",
    "\"Tu est.\",\n",
    "\"Il est.\",\n",
    "\"Elle est.\",\n",
    "\"Nous sommes.\",\n",
    "\"Ils sont.\"]\n",
    "gt_fr=[\"Je ne peux pas te permettre de faire ça.\",\n",
    "\"Tu penses que je suis moche, n'est-ce pas?\",\n",
    "\"Comment oses-tu dire une chose pareille!\",\n",
    "\"Je le comprends, mais je ne suis toujours pas d'accord avec cela.\",\n",
    "\"C'est une excellente journée pour une randonnée.\",\n",
    "\"Le modérateur était David Ignatius du Washington Post.\",\n",
    "\"Il appartient à la tribu Mehsud de la région troublée du Sud-Waziristan au Pakistan et commande environ 20 000 militants pro-talibans.\",\n",
    "\"Sans eau, les filles doivent souvent parcourir de longues distances en transportant de l’eau lourde.\",\n",
    "\"Même le Ghana et le Laos nous ont dépassés l’année dernière.\",\n",
    "\"Mais à partir du mardi 14 avril 2009, Neptune quittera progressivement Lakhana Duang Muang vers la droite du Soleil. Cela signifie que les mauvais présages ou les mauvais incidents contre Bangkok seront subventionnés.\",\n",
    "\"Je suis.\",\n",
    "\"You are.\",\n",
    "\"Il est.\",\n",
    "\"Elle est.\",\n",
    "\"Nous sommes.\",\n",
    "\"Ils ont.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "score=0\n",
    "for i in range(16):\n",
    "    s=(bleu_score([spacy_fr(gt_en[i])],[[spacy_fr(gt_fr[i])]],max_n=2,weights=[1/2,1/2]))\n",
    "\n",
    "    print(s)\n",
    "    score+=s\n",
    "print(score/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9decea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bleu_score([['Ils', 'sont', '.']], [[['Ils', 'sont', '.']]],max_n=1,weights=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd92e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bleu_score(spacy_fr(gt_en[15]),spacy_fr(gt_fr[15]),max_n=1,weights=[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c2874",
   "metadata": {},
   "outputs": [],
   "source": [
    "(spacy_fr(gt_fr[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57085f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
